{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2a8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os, io, re, tqdm, datetime, nltk, torch, transformers #nltk mecbur olmasaz yuklemeyin\n",
    "\n",
    "import pandas as pd, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from torch import nn\n",
    "from torchtext import vocab\n",
    "from transformers import BertTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.optim import Adam, Adagrad, RMSprop, SGD\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action= 'ignore')\n",
    "\n",
    "#set parameters for \"Matplotlib\"\n",
    "mpl.rcParams['figure.figsize']  = [16, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1871a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# pip uninstall torch torchtext -y\n",
    "# pip install torch==2.1.0 torchtext==0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038a933",
   "metadata": {},
   "source": [
    "### About Dataset\n",
    "#### AG's News Topic Classification Dataset\n",
    "### ORIGIN\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
    "\n",
    "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
    "\n",
    "### DESCRIPTION\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n",
    "\n",
    "The file classes.txt contains a list of classes corresponding to each label.\n",
    "\n",
    "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 4), title and description. The title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
    "\n",
    "#### About this file\n",
    "\n",
    "This file consists of 7600 testing samples of news articles that contain 3 columns. The first column is Class Id, the second column is Title and the third column is Description. \n",
    "\n",
    "**The class ids are numbered 1-4 where**\n",
    "* 1 ------------> World\n",
    "* 2 ------------> Sports\n",
    "* 3 ------------> Business\n",
    "* 4 ------------> Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee10cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "df = pd.concat([train_df, test_df], axis= 0)\n",
    "df.index = range(df.shape[0])\n",
    "df['text'] = 'The title of news is ' + df['Title'] + ' .The news are below ' +  df['Description']\n",
    "df.rename({'Class Index': 'label'}, axis= 1, inplace= True)\n",
    "df.drop(['Title', 'Description'], axis= 1, inplace= True)\n",
    "df = df[['text', 'label']]\n",
    "df['label'] = df['label'] - 1\n",
    "# df.to_csv('Final_df.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67bf4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.len().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f5f964",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[\u001b[43midx\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "df.loc[idx, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdab6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e698632",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PATH = 'bert-base-uncased'\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
    "vectorized_text = TOKENIZER.encode_plus(df.loc[idx, 'text'], None,\n",
    "                   add_special_tokens=True,\n",
    "                   max_length = 192,\n",
    "                   pad_to_max_length=True,\n",
    "                   truncation='longest_first',), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe77613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [101, 1996, 2516, 1997, 2739, 2003, 2678, 2399, 1005, 2204, 2005, 2336, 1005, 1012, 1996, 2739, 2024, 2917, 3274, 2399, 2064, 5326, 3291, 1011, 13729, 1998, 2136, 1011, 2311, 1999, 2336, 1010, 2360, 2399, 3068, 8519, 1012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb03e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a11346",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(vectorized_text[0]['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d6a9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GetDataset at 0x160171b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_PATH = 'bert-base-uncased'\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
    "#reqemlere cevirsin deye\n",
    "\n",
    "class GetDataset():\n",
    "    def __init__(self, dataset, tokenizer, MAX_LEN):\n",
    "        self.label = dataset['label']\n",
    "        self.text = dataset['text']\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label) #tells pytorch how many rows does dataframe have\n",
    "    \n",
    "    def __getitem__(self, idx): #iterable-style dataset and map-style - __iter__ & __len__ \n",
    "        label = self.label.iloc[idx]\n",
    "        text = self.text.iloc[idx]\n",
    "        vectorized_text = TOKENIZER.encode_plus(text, None,\n",
    "                                               add_special_tokens= True,\n",
    "                                               max_length = self.max_len,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               truncation='longest_first',), \n",
    "        input_ids = vectorized_text[0]['input_ids']\n",
    "        attention_masks = vectorized_text[0]['attention_mask']\n",
    "#         tokenized_text = tokenizer(text)\n",
    "        \n",
    "        \n",
    "        return {'ids': torch.tensor(input_ids),\n",
    "                'attention_masks': attention_masks,\n",
    "                'labels': torch.tensor(label)}\n",
    "    \n",
    "dataset = GetDataset(df, tokenizer= TOKENIZER, MAX_LEN= 192)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196f729",
   "metadata": {},
   "source": [
    "## üìå Key Parameters\n",
    "\n",
    "### üîπ Short Explanation\n",
    "\n",
    "- **`vocab_size`**: Number of unique words (tokens) in your dataset's vocabulary.\n",
    "- **`embed_dim`**: Size of each word embedding vector (e.g., 100, 300).\n",
    "- **`num_class`**: Number of output categories (labels) for classification.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Detailed Explanation\n",
    "\n",
    "#### 1. üßæ `vocab_size` ‚Äî Vocabulary Size\n",
    "- Refers to the **total number of unique tokens** (words, subwords, or characters) in your dataset.\n",
    "- After tokenization, you build a vocabulary.  \n",
    "- Example: If you have 10,000 unique words, then `vocab_size = 10000`.\n",
    "- In `nn.Embedding`, this tells PyTorch **how many rows** to create in the embedding matrix (one for each token).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. üìè `embed_dim` ‚Äî Embedding Dimension\n",
    "- Each token is converted into a **dense vector** of fixed size.\n",
    "- `embed_dim` defines **how many features** each word is represented by.\n",
    "- Common values: `50`, `100`, `300`, `768` (used in BERT).\n",
    "- If `embed_dim = 100`, then:\n",
    "  - \"king\" ‚Üí `[0.12, -0.45, ..., 0.33]` (a 100-dimensional vector)\n",
    "\n",
    "‚úÖ Embeddings help capture **semantic meaning** ‚Äî similar words have similar vectors.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. üéØ `num_class` ‚Äî Number of Output Classes\n",
    "- Defines the number of **categories/labels** your model predicts.\n",
    "  - For binary classification ‚Üí `num_class = 2`\n",
    "  - For sentiment classification (pos/neg/neutral) ‚Üí `num_class = 3`\n",
    "  - For topic classification ‚Üí `num_class = N`\n",
    "\n",
    "- The final layer is often:\n",
    "  ```python\n",
    "  nn.Linear(in_features, num_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706af02b",
   "metadata": {},
   "source": [
    "## Embedding Dimensonlarda reqem nece yaranir? (Embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6d6e7",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4824d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Final_df.csv'\n",
    "tokenizer = get_tokenizer('basic_english') #sozlere ayirmaq ucun\n",
    "\n",
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path) as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line.strip())\n",
    "            \n",
    "vocab = build_vocab_from_iterator(yield_tokens(file_path), specials= ['<unk>'])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f85c4889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewsClassificationGRU(\n",
       "  (embedding_layer): Embedding(98628, 64)\n",
       "  (transformer_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (gru_layer): GRU(64, 128, num_layers=2, batch_first=True)\n",
       "  (dense_layer): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (softmax_act_fc): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_transformer_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=768,  # embedding size\n",
    "    nhead=12      # number of attention heads\n",
    ")\n",
    "\n",
    "class NewsClassificationGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, num_heads):\n",
    "        super(NewsClassificationGRU, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(num_embeddings= vocab_size, embedding_dim= embed_dim)\n",
    "        \n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "                            d_model= embed_dim, \n",
    "                            nhead= num_heads\n",
    "        )\n",
    "        \n",
    "        self.gru_layer = nn.GRU(embed_dim, hidden_size= 128, num_layers= 2, batch_first= True)  #RNN, GRU, LSTM - sequence tipli datalarla ishlemek ucun\n",
    "                                                                             #istifade olunur. Sequence - NLP ve Time Series\n",
    "        self.dense_layer = nn.Linear(128, num_class)\n",
    "        self.softmax_act_fc = nn.Softmax(dim= 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        X = self.embedding_layer(input_ids)\n",
    "        X = X.transpose(0, 1)   # [seq_len, batch, embed_dim]\n",
    "        mask = (attention_mask == 0)   # shape should be [batch, seq_len]\n",
    "        if mask.shape[0] != X.shape[1]:   # X.shape[1] = batch\n",
    "            mask = mask.transpose(0, 1)   # fix if it‚Äôs swapped accidentally\n",
    "\n",
    "        X = self.transformer_layer(X, src_key_padding_mask=mask)\n",
    "#         X = self.transformer_layer(\n",
    "#             X,\n",
    "#             src_key_padding_mask=(attention_mask == 0)   # [batch, seq_len] ‚úÖ\n",
    "#         )\n",
    "\n",
    "        X = X.transpose(0, 1)   \n",
    "        \n",
    "#   \n",
    "        X, _ = self.gru_layer(X)              # batch_first=True ‚Üí [batch, seq_len, hidden_size]\n",
    "        X = X[:, -1, :]                    # last time step [batch, hidden_size]\n",
    "        X = self.dense_layer(X) \n",
    "# #         X, _ = self.gru_layer(X) #return tuple - 2 dene shey qaytarir\n",
    "# #         X = self.dense_layer(X)\n",
    "# #         X = X[:, -1 ,:]\n",
    "#         X = self.softmax_act_fc(X)\n",
    "        return X\n",
    "    \n",
    "model = NewsClassificationGRU(vocab_size= vocab_size, \n",
    "                              embed_dim= 64, \n",
    "                              num_class= 4, \n",
    "                              num_heads= 32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3e94a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(ids, attention_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638691e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.uns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab8e1f95",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NewsClassificationGRU.forward() missing 2 required positional arguments: 'input_ids' and 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: NewsClassificationGRU.forward() missing 2 required positional arguments: 'input_ids' and 'attention_mask'"
     ]
    }
   ],
   "source": [
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64cf8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 3\n",
    "NUM_WORKERS = 0\n",
    "optimizer = SGD(model.parameters(), lr= LEARNING_RATE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_size = int(df.shape[0] * 0.9)\n",
    "train_subset, test_subset = random_split(dataset, lengths= [train_size, df.shape[0] - train_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_subset, batch_size= BATCH_SIZE, num_workers = NUM_WORKERS )\n",
    "test_data_loader = DataLoader(test_subset, batch_size= BATCH_SIZE, num_workers= NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366ce8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6de93c32",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ecce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98000d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76436337",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71775fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f452f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "377005d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ----------------------------->\n",
      "\n",
      "Loss for batch 100: 1.39\n",
      "Loss for batch 200: 1.38\n",
      "Loss for batch 300: 1.38\n",
      "Loss for batch 400: 1.38\n",
      "Loss for batch 500: 1.39\n",
      "Loss for batch 600: 1.38\n",
      "Loss for batch 700: 1.38\n",
      "Loss for batch 800: 1.39\n",
      "Epoch 2 ----------------------------->\n",
      "\n",
      "Loss for batch 100: 1.38\n",
      "Loss for batch 200: 1.38\n",
      "Loss for batch 300: 1.38\n",
      "Loss for batch 400: 1.38\n",
      "Loss for batch 500: 1.38\n",
      "Loss for batch 600: 1.38\n",
      "Loss for batch 700: 1.38\n",
      "Loss for batch 800: 1.38\n",
      "Epoch 3 ----------------------------->\n",
      "\n",
      "Loss for batch 100: 1.36\n",
      "Loss for batch 200: 1.33\n",
      "Loss for batch 300: 1.32\n",
      "Loss for batch 400: 1.34\n",
      "Loss for batch 500: 1.26\n",
      "Loss for batch 600: 1.24\n",
      "Loss for batch 700: 1.2\n",
      "Loss for batch 800: 1.16\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch_n in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch_n} ----------------------------->\\n')\n",
    "    for idx, (batch) in enumerate(train_data_loader):\n",
    "        ids = batch['ids']\n",
    "        attention_mask = batch['attention_masks']\n",
    "        attention_mask = torch.stack(attention_mask, dim=0)\n",
    "        \n",
    "        labels = batch['labels']\n",
    "        \n",
    "        predictions = model(ids, attention_mask)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        #backward propogation\n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        if idx % 100 == 0 and idx != 0:\n",
    "            print(f\"Loss for batch {idx}: {round(loss.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc3e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530678d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c3d3408",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "489bce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value: 1388 out of 3200\n",
      "Accuracy Value: 2705 out of 6400\n",
      "Accuracy Value: 4070 out of 9600\n",
      "\n",
      "Accuracy Score: 41.71\n"
     ]
    }
   ],
   "source": [
    "accuracy_value = 0\n",
    "test_size = df.shape[0] - train_size\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_data_loader):\n",
    "        ids = batch['ids']\n",
    "        attention_mask = batch['attention_masks']\n",
    "        attention_mask = torch.stack(attention_mask, dim=0)\n",
    "        \n",
    "        labels = batch['labels']\n",
    "        \n",
    "        predictions = model(ids, attention_mask)\n",
    "        accuracy_value += (predictions.argmax(1) == labels).sum().item()\n",
    "        if idx % 25 == 0 and idx != 0:  \n",
    "            print(f\"Accuracy Value: {accuracy_value} out of {idx  * BATCH_SIZE}\") \n",
    "\n",
    "    print(f\"\\nAccuracy Score: {round((accuracy_value / test_size) * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a2a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
